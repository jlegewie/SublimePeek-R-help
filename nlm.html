<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
 <html><head><title>R: Non-Linear Minimization</title>
 <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
 <link rel="stylesheet" type="text/css" href="R.css">
 </head><body>
 
 <table width="100%" summary="page for nlm"><tr><td>nlm</td><td align="right">R Documentation</td></tr></table>
 
 <h2>Non-Linear Minimization</h2>
 
 <h3>Description</h3>
 
 
 <p>This function carries out a minimization of the function <code>f</code>
 using a Newton-type algorithm.  See the references for details.
 </p>
 
 
 <h3>Usage</h3>
 
 <pre>
 nlm(f, p, ..., hessian = FALSE, typsize = rep(1, length(p)),
     fscale = 1, print.level = 0, ndigit = 12, gradtol = 1e-6,
     stepmax = max(1000 * sqrt(sum((p/typsize)^2)), 1000),
     steptol = 1e-6, iterlim = 100, check.analyticals = TRUE)
 </pre>
 
 
 <h3>Arguments</h3>
 
 
 <table summary="R argblock">
 <tr valign="top"><td><code>f</code></td>
 <td>
 <p>the function to be minimized.  If the function value has
 an attribute called <code>gradient</code> or both <code>gradient</code> and
 <code>hessian</code> attributes, these will be used in the calculation of
 updated parameter values.  Otherwise, numerical derivatives are
 used. <code>deriv</code> returns a function with suitable
 <code>gradient</code> attribute.  This should be a function of a vector of
 the length of <code>p</code> followed by any other arguments specified
 by the <code>...</code> argument.</p>
 </td></tr>
 <tr valign="top"><td><code>p</code></td>
 <td>
 <p>starting parameter values for the minimization.</p>
 </td></tr>
 <tr valign="top"><td><code>...</code></td>
 <td>
 <p>additional arguments to <code>f</code>.</p>
 </td></tr>
 <tr valign="top"><td><code>hessian</code></td>
 <td>
 <p>if <code>TRUE</code>, the hessian of <code>f</code>
 at the minimum is returned.</p>
 </td></tr>
 <tr valign="top"><td><code>typsize</code></td>
 <td>
 <p>an estimate of the size of each parameter
 at the minimum.</p>
 </td></tr>
 <tr valign="top"><td><code>fscale</code></td>
 <td>
 <p>an estimate of the size of <code>f</code> at the minimum.</p>
 </td></tr>
 <tr valign="top"><td><code>print.level</code></td>
 <td>
 <p>this argument determines the level of printing
 which is done during the minimization process.  The default
 value of <code>0</code> means that no printing occurs, a value of <code>1</code>
 means that initial and final details are printed and a value
 of 2 means that full tracing information is printed.</p>
 </td></tr>
 <tr valign="top"><td><code>ndigit</code></td>
 <td>
 <p>the number of significant digits in the function <code>f</code>.</p>
 </td></tr>
 <tr valign="top"><td><code>gradtol</code></td>
 <td>
 <p>a positive scalar giving the tolerance at which the
 scaled gradient is considered close enough to zero to
 terminate the algorithm.  The scaled gradient is a
 measure of the relative change in <code>f</code> in each direction
 <code>p[i]</code> divided by the relative change in <code>p[i]</code>.</p>
 </td></tr>
 <tr valign="top"><td><code>stepmax</code></td>
 <td>
 <p>a positive scalar which gives the maximum allowable
 scaled step length.  <code>stepmax</code> is used to prevent steps which
 would cause the optimization function to overflow, to prevent the
 algorithm from leaving the area of interest in parameter space, or to
 detect divergence in the algorithm. <code>stepmax</code> would be chosen
 small enough to prevent the first two of these occurrences, but should
 be larger than any anticipated reasonable step.</p>
 </td></tr>
 <tr valign="top"><td><code>steptol</code></td>
 <td>
 <p>A positive scalar providing the minimum allowable
 relative step length.</p>
 </td></tr>
 <tr valign="top"><td><code>iterlim</code></td>
 <td>
 <p>a positive integer specifying the maximum number of
 iterations to be performed before the program is terminated.</p>
 </td></tr>
 <tr valign="top"><td><code>check.analyticals</code></td>
 <td>
 <p>a logical scalar specifying whether the
 analytic gradients and Hessians, if they are supplied, should be
 checked against numerical derivatives at the initial parameter
 values. This can help detect incorrectly formulated gradients or
 Hessians.</p>
 </td></tr>
 </table>
 
 
 <h3>Details</h3>
 
 
 <p>Note that arguments after <code>...</code> must be matched exactly.
 </p>
 <p>If a gradient or hessian is supplied but evaluates to the wrong mode
 or length, it will be ignored if <code>check.analyticals = TRUE</code> (the
 default) with a warning.  The hessian is not even checked unless the
 gradient is present and passes the sanity checks.
 </p>
 <p>From the three methods available in the original source, we always use
 method &ldquo;1&rdquo; which is line search.
 </p>
 <p>The functions supplied must always return finite (including not
 <code>NA</code> and not <code>NaN</code>) values.
 </p>
 
 
 <h3>Value</h3>
 
 
 <p>A list containing the following components:
 </p>
 <table summary="R valueblock">
 <tr valign="top"><td><code>minimum</code></td>
 <td>
 <p>the value of the estimated minimum of <code>f</code>.</p>
 </td></tr>
 <tr valign="top"><td><code>estimate</code></td>
 <td>
 <p>the point at which the minimum value of
 <code>f</code> is obtained.</p>
 </td></tr>
 <tr valign="top"><td><code>gradient</code></td>
 <td>
 <p>the gradient at the estimated minimum of <code>f</code>.</p>
 </td></tr>
 <tr valign="top"><td><code>hessian</code></td>
 <td>
 <p>the hessian at the estimated minimum of <code>f</code> (if
 requested).</p>
 </td></tr>
 <tr valign="top"><td><code>code</code></td>
 <td>
 <p>an integer indicating why the optimization process terminated.
 </p>
 
 <dl>
 <dt>1:</dt><dd><p>relative gradient is close to zero, current iterate is
 probably solution.</p>
 </dd>
 <dt>2:</dt><dd><p>successive iterates within tolerance, current iterate
 is probably solution.</p>
 </dd>
 <dt>3:</dt><dd><p>last global step failed to locate a point lower than
 <code>estimate</code>.  Either <code>estimate</code> is an approximate local
 minimum of the function or <code>steptol</code> is too small.</p>
 </dd>
 <dt>4:</dt><dd><p>iteration limit exceeded.</p>
 </dd>
 <dt>5:</dt><dd><p>maximum step size <code>stepmax</code> exceeded five consecutive
 times.  Either the function is unbounded below,
 becomes asymptotic to a finite value from above in
 some direction or <code>stepmax</code> is too small.</p>
 </dd>
 </dl>
 
 </td></tr>
 <tr valign="top"><td><code>iterations</code></td>
 <td>
 <p>the number of iterations performed.</p>
 </td></tr>
 </table>
 
 
 <h3>References</h3>
 
 
 <p>Dennis, J. E. and Schnabel, R. B. (1983) <EM>Numerical Methods for
 Unconstrained Optimization and Nonlinear Equations.</EM> Prentice-Hall,
 Englewood Cliffs, NJ.
 </p>
 <p>Schnabel, R. B., Koontz, J. E. and Weiss, B. E. (1985) A modular
 system of algorithms for unconstrained minimization.
 <EM>ACM Trans. Math. Software</EM>, <B>11</B>, 419&ndash;440.
 </p>
 
 
 <h3>See Also</h3>
 
 
 <p><code>optim</code> and <code>nlminb</code>.
 </p>
 <p><code>constrOptim</code> for constrained optimization, 
 <code>optimize</code> for one-dimensional
 minimization and <code>uniroot</code> for root finding.
 <code>deriv</code> to calculate analytical derivatives.
 </p>
 <p>For nonlinear regression, <code>nls</code> may be better.
 </p>
 
 
 <h3>Examples</h3>
 
 <pre>
 f &lt;- function(x) sum((x-1:length(x))^2)
 nlm(f, c(10,10))
 nlm(f, c(10,10), print.level = 2)
 utils::str(nlm(f, c(5), hessian = TRUE))
 
 f &lt;- function(x, a) sum((x-a)^2)
 nlm(f, c(10,10), a=c(3,5))
 f &lt;- function(x, a)
 {
     res &lt;- sum((x-a)^2)
     attr(res, "gradient") &lt;- 2*(x-a)
     res
 }
 nlm(f, c(10,10), a=c(3,5))
 
 ## more examples, including the use of derivatives.
 ## Not run: demo(nlm)
 </pre>
 
 
 </body></html>
